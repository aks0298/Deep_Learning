{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77677ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be90d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd08ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "X=np.random.uniform(-2, 2, (400, 3))\n",
    "y=(np.sin(X[:,0])+0.5*(X[:,1]**2)-0.8*X[:,2])\n",
    "y=y.reshape(-1,1)\n",
    "\n",
    "X=X.T\n",
    "y=y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1627c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A - Shallow   [3,4,1]\n",
      "  Layer 1: 4x3 + 4 = 16\n",
      "  Layer 2: 1x4 + 1 = 5\n",
      "  Total: 21\n",
      "\n",
      "Model B - Medium    [3,6,6,1]\n",
      "  Layer 1: 6x3 + 6 = 24\n",
      "  Layer 2: 6x6 + 6 = 42\n",
      "  Layer 3: 1x6 + 1 = 7\n",
      "  Total: 73\n",
      "\n",
      "Model C - Deep      [3,8,8,8,8,1]\n",
      "  Layer 1: 8x3 + 8 = 32\n",
      "  Layer 2: 8x8 + 8 = 72\n",
      "  Layer 3: 8x8 + 8 = 72\n",
      "  Layer 4: 8x8 + 8 = 72\n",
      "  Layer 5: 1x8 + 1 = 9\n",
      "  Total: 257\n",
      "\n",
      "Model D - VeryDeep  [3,8x8,1]\n",
      "  Layer 1: 8x3 + 8 = 32\n",
      "  Layer 2: 8x8 + 8 = 72\n",
      "  Layer 3: 8x8 + 8 = 72\n",
      "  Layer 4: 8x8 + 8 = 72\n",
      "  Layer 5: 8x8 + 8 = 72\n",
      "  Layer 6: 8x8 + 8 = 72\n",
      "  Layer 7: 8x8 + 8 = 72\n",
      "  Layer 8: 8x8 + 8 = 72\n",
      "  Layer 9: 1x8 + 1 = 9\n",
      "  Total: 545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_params(dims):\n",
    "    total=0\n",
    "    for i in range(1,len(dims)):\n",
    "        p=dims[i]*dims[i-1]+dims[i]\n",
    "        print(f\"  Layer {i}: {dims[i]}x{dims[i-1]} + {dims[i]} = {p}\")\n",
    "        total+=p\n",
    "    print(f\"  Total: {total}\\n\")\n",
    "\n",
    "models={\n",
    "    \"Model A - Shallow   [3,4,1]\":[3,4,1],\n",
    "    \"Model B - Medium    [3,6,6,1]\":[3,6,6,1],\n",
    "    \"Model C - Deep      [3,8,8,8,8,1]\":[3,8,8,8,8,1],\n",
    "    \"Model D - VeryDeep  [3,8x8,1]\":[3,8,8,8,8,8,8,8,8,1],\n",
    "}\n",
    "\n",
    "for name,dims in models.items():\n",
    "    print(name)\n",
    "    count_params(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c7b090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):        \n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_d(z):      \n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):     \n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_fn(z):    \n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_d(z):      \n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "ALPHA = 0.01   \n",
    "def leaky_relu(z):   \n",
    "    return np.where(z > 0, z, ALPHA * z)\n",
    "\n",
    "def leaky_relu_d(z): \n",
    "    return np.where(z > 0, 1.0, ALPHA)\n",
    "\n",
    "def softplus(z):    \n",
    "    return np.log1p(np.exp(np.clip(z, -500, 500)))\n",
    "\n",
    "def softplus_d(z):  \n",
    "    return sigmoid(z)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d0c72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTS={\n",
    "    \"relu\":(relu,relu_d),\n",
    "    \"sigmoid\":(sigmoid,sigmoid_d),\n",
    "    \"tanh\":(tanh_fn,tanh_d),\n",
    "    \"leaky_relu\":(leaky_relu,leaky_relu_d),\n",
    "    \"softplus\":(softplus,softplus_d),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf520e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(dims):\n",
    "    params=[]\n",
    "    for i in range(1,len(dims)):\n",
    "        W=np.random.uniform(-0.5,0.5,(dims[i],dims[i-1]))\n",
    "        b=np.zeros((dims[i],1))\n",
    "        params.append((W,b))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea7fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,params,act):\n",
    "    cache=[]\n",
    "    A=X\n",
    "    for i,(W,b) in enumerate(params):\n",
    "        Z=W@A+b\n",
    "        A_prev=A\n",
    "        if i==len(params)-1:\n",
    "            A=Z\n",
    "        else:\n",
    "            A=act(Z)\n",
    "        cache.append((A_prev,Z))\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "060dcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(pred,true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def mse_grad(pred,true):\n",
    "    N=true.shape[1]\n",
    "    return (2/N)*(pred-true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abddf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA,params,cache,act_d):\n",
    "    grads=[None]*len(params)\n",
    "    for i in reversed(range(len(params))):\n",
    "        W,b=params[i]\n",
    "        A_prev,Z=cache[i]\n",
    "        N=A_prev.shape[1]\n",
    "        if i==len(params)-1:\n",
    "            dZ=dA\n",
    "        else:\n",
    "            dZ=dA*act_d(Z)\n",
    "        dW=(1/N)*(dZ@A_prev.T)\n",
    "        db=(1/N)*np.sum(dZ,axis=1,keepdims=True)\n",
    "        dA=W.T@dZ\n",
    "        grads[i]=(dW,db)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ec30975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params,grads,lr):\n",
    "    return[(W-lr*dW,b-lr*db)for (W,b),(dW,db) in zip(params,grads)]\n",
    "\n",
    "def grad_norm(dW):\n",
    "    return np.sqrt(np.sum(dW**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e24d97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dims,act_name=\"relu\",lr=0.01,epochs=1000,verbose=True):\n",
    "    act,act_d=ACTS[act_name]\n",
    "    params=init(dims)\n",
    "    history=[]\n",
    "    for ep in range(1,epochs+1):\n",
    "        pred,cache=forward(X,params,act)\n",
    "        loss=mse(pred,y)\n",
    "        history.append(loss)\n",
    "        grads=backward(mse_grad(pred,y),params,cache,act_d)\n",
    "        params=update(params,grads,lr)\n",
    "        if verbose and (ep%200==0 or ep==1):\n",
    "            print(f\"  Epoch {ep:4d} | Loss: {loss:.5f}\")\n",
    "    pred,cache=forward(X,params,act)\n",
    "    grads=backward(mse_grad(pred,y),params,cache,act_d)\n",
    "    n_hidden=len(params)-1\n",
    "    return{\n",
    "        \"history\":history,\n",
    "        \"final\":history[-1],\n",
    "        \"ep200\":history[199],\n",
    "        \"gn_first\":grad_norm(grads[0][0]),\n",
    "        \"gn_last\":grad_norm(grads[n_hidden-1][0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3c65f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Model A - Shallow | relu\n",
      "  Epoch    1 | Loss: 1.87697\n",
      "  Epoch  200 | Loss: 1.86785\n",
      "  Epoch  400 | Loss: 1.85874\n",
      "  Epoch  600 | Loss: 1.84970\n",
      "  Epoch  800 | Loss: 1.84074\n",
      "  Epoch 1000 | Loss: 1.83184\n",
      "\n",
      ">> Model B - Medium | relu\n",
      "  Epoch    1 | Loss: 2.07834\n",
      "  Epoch  200 | Loss: 2.06590\n",
      "  Epoch  400 | Loss: 2.05373\n",
      "  Epoch  600 | Loss: 2.04188\n",
      "  Epoch  800 | Loss: 2.03035\n",
      "  Epoch 1000 | Loss: 2.01911\n",
      "\n",
      ">> Model C - Deep 4h | relu\n",
      "  Epoch    1 | Loss: 2.17555\n",
      "  Epoch  200 | Loss: 2.16488\n",
      "  Epoch  400 | Loss: 2.15443\n",
      "  Epoch  600 | Loss: 2.14426\n",
      "  Epoch  800 | Loss: 2.13436\n",
      "  Epoch 1000 | Loss: 2.12473\n",
      "\n",
      ">> Model D - Deep 8h | relu\n",
      "  Epoch    1 | Loss: 2.16085\n",
      "  Epoch  200 | Loss: 2.14992\n",
      "  Epoch  400 | Loss: 2.13951\n",
      "  Epoch  600 | Loss: 2.12959\n",
      "  Epoch  800 | Loss: 2.11996\n",
      "  Epoch 1000 | Loss: 2.11067\n",
      "\n",
      ">> Model D - Deep 8h | sigmoid\n",
      "  Epoch    1 | Loss: 2.37204\n",
      "  Epoch  200 | Loss: 2.33711\n",
      "  Epoch  400 | Loss: 2.30408\n",
      "  Epoch  600 | Loss: 2.27301\n",
      "  Epoch  800 | Loss: 2.24377\n",
      "  Epoch 1000 | Loss: 2.21626\n",
      "Model                  Act         FinalLoss   Loss@200   GradNorm_L1  GradNorm_Last\n",
      "Model A - Shallow      relu          1.83184    1.86785      0.002014       0.002014\n",
      "Model B - Medium       relu          2.01911    2.06590      0.000939       0.001373\n",
      "Model C - Deep 4h      relu          2.12473    2.16488      0.000194       0.000355\n",
      "Model D - Deep 8h      relu          2.11067    2.14992      0.000059       0.000134\n",
      "Model D - Deep 8h      sigmoid       2.21626    2.33711      0.000000       0.000777\n"
     ]
    }
   ],
   "source": [
    "configs=[\n",
    "    (\"Model A - Shallow\",[3,4,1],\"relu\"),\n",
    "    (\"Model B - Medium\",[3,6,6,1],\"relu\"),\n",
    "    (\"Model C - Deep 4h\",[3,8,8,8,8,1],\"relu\"),\n",
    "    (\"Model D - Deep 8h\",[3,8,8,8,8,8,8,8,8,1],\"relu\"),\n",
    "    (\"Model D - Deep 8h\",[3,8,8,8,8,8,8,8,8,1],\"sigmoid\"),\n",
    "]\n",
    "\n",
    "table=[]\n",
    "for name,dims,act in configs:\n",
    "    print(f\"\\n>> {name} | {act}\")\n",
    "    r=train(dims,act)\n",
    "    table.append((name,act,r[\"final\"],r[\"ep200\"],r[\"gn_first\"],r[\"gn_last\"]))\n",
    "\n",
    "print(f\"{'Model':<22} {'Act':<10} {'FinalLoss':>10} {'Loss@200':>10} {'GradNorm_L1':>13} {'GradNorm_Last':>14}\")\n",
    "for row in table:\n",
    "    print(f\"{row[0]:<22} {row[1]:<10} {row[2]:>10.5f} {row[3]:>10.5f} {row[4]:>13.6f} {row[5]:>14.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1185e",
   "metadata": {},
   "source": [
    "Q1. Did deeper networks always reduce the loss faster?\n",
    "ans : not always. model A acheived the lowest final loss , as the depth of the model increases the final loss actually became worse for this case.\n",
    "\n",
    "Q2.Did gradients stay similar across layers?\n",
    "ans : no , as the network became deeper the gradient norm in the 1st layer decreased significantly -> vanishing gradient problem \n",
    "\n",
    "Q3. Was training equally stable for all activations?\n",
    "ans: no the deep model with sigmoid had the highest loss which indicates there is unstability and slow learning as compared to Relu\n",
    "\n",
    "Q4. Which activation was more stable in deep networks?\n",
    "ans : in this case Relu was more stable as compared to Sigmoid \n",
    "\n",
    "Q5. Did some models improve very slowly despite using the same learning rate?\n",
    "ans : yes deep 8 layer model with sigmoid activation improved slowly ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
